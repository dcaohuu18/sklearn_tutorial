{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('winequality-red.csv', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this tutorial doesn't discuss exploratory data analysis in depth but it's a vital part of real-world machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate target from training features:\n",
    "y = data.quality\n",
    "X = data.loc[:, :'alcohol'] # or: X = data.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: It's good practice to **stratify your sample** by the target variable. This will ensure your training set looks similar to your test set, making your evaluation metrics more reliable.\n",
    "\n",
    "**Definition:**: Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should define a partition of the population. Then simple random sampling or systematic sampling is applied within each stratum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare data preprocessing steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** \n",
    "\n",
    "Standardization is the process of subtracting the mean from each feature value and then dividing by the standard deviation.\n",
    "\n",
    "Standardization is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance. This is to ensure that all features follow the same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -0. -0. -0.  0. -0. -0. -0. -0. -0. -0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Notice: we won't actually use the following code!\n",
    "\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "print(np.round(X_train_scaled.mean(axis=0)))\n",
    "print(X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the scaled dataset is centered at approximately zero, with unit variance (1).\n",
    "\n",
    "---\n",
    "\n",
    "**Standard Normal Distribution:** The standard normal distribution is a special case of the normal distribution. It is the distribution that occurs when a normal random variable has a **mean of zero** and a **standard deviation of one**.\n",
    "\n",
    "The normal random variable of a standard normal distribution is called a standard score or a $z$ score. Every normal random variable $X$ can be transformed into a $z$ score via the following equation: $$z = (X - \\mu) / \\sigma$$\n",
    "\n",
    "where $X$ is a normal random variable, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "<img src=\"https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module6-RandomError/Normal%20Distribution%20deviations.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "\n",
    "The reason why we won't use the code above (cell 7) is that we won't be able to perform the exact same transformation on the test set.\n",
    "\n",
    "We can still scale the test set separately, but we won't be using the same means and standard deviations as we used to transform the training set. That means it wouldn't be a fair representation of how the model pipeline, including the preprocessing steps, would perform on brand new data.\n",
    "\n",
    "Instead of directly invoking the ```scale``` function, we'll be using a feature in Scikit-Learn called the Transformer API. The Transformer API allows you to \"fit\" a preprocessing step using the training data the same way you'd fit a model and then use the same transformation on future data sets!\n",
    "\n",
    "Here's what that process looks like:\n",
    "1. Fit the transformer on the training set (saving the means and standard deviations)\n",
    "2. Apply the transformer to the training set (scaling the training data)\n",
    "3. Apply the transformer to the test set (using the same means and standard deviations)\n",
    "\n",
    "This makes your final estimate of model performance more realistic, and it allows to insert your preprocessing steps into a cross-validation pipeline (more details below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Transformer API:\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the scaler object has the saved means and standard deviations for each feature in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -0. -0. -0.  0. -0. -0. -0. -0. -0. -0.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(np.round(X_train_scaled.mean(axis=0)))\n",
    "print(X_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform the test set using the exact same means and standard deviations used to transform the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
      " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n",
      "[1.02160495 1.00135689 0.97456598 0.91099054 0.86716698 0.94193125\n",
      " 1.03673213 1.03145119 0.95734849 0.83829505 1.0286218 ]\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    " \n",
    "print(X_test_scaled.mean(axis=0))\n",
    "print(X_test_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The scaled features in the test set are not perfectly centered at zero with unit variance! This is exactly what we'd expect, as we're transforming the test set using the means from the training set, not from the test set itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, when we set up the cross-validation pipeline, we won't even need to manually fit the Transformer API. Instead, we'll simply declare the class object, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what it looks like: a modeling pipeline that first transforms the data using ```StandardScaler()``` and then fits a model using a random forest regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare hyperparameters to tune:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** There are two types of parameters we need to worry about: model parameters and hyperparameters. Models parameters can be learned directly from the data (e.g. regression coefficients), while hyperparameters cannot.\n",
    "\n",
    "Hyperparameters express \"higher-level\" structural information about the model, and they are typically set before training the model (e.g. polynomial degree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest hyperparameters:**\n",
    "Within each decision tree, the computer can empirically decide where to create branches based on either mean-squared-error (MSE) or mean-absolute-error (MAE). The actual branch locations are model parameters.\n",
    "\n",
    "However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide how many trees to include in the forest. These are examples of hyperparameters that the user must set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('randomforestregressor',\n",
       "   RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         max_samples=None, min_impurity_decrease=0.0,\n",
       "                         min_impurity_split=None, min_samples_leaf=1,\n",
       "                         min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                         n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                         random_state=None, verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'randomforestregressor': RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       max_samples=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=None, verbose=0, warm_start=False),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'randomforestregressor__bootstrap': True,\n",
       " 'randomforestregressor__ccp_alpha': 0.0,\n",
       " 'randomforestregressor__criterion': 'mse',\n",
       " 'randomforestregressor__max_depth': None,\n",
       " 'randomforestregressor__max_features': 'auto',\n",
       " 'randomforestregressor__max_leaf_nodes': None,\n",
       " 'randomforestregressor__max_samples': None,\n",
       " 'randomforestregressor__min_impurity_decrease': 0.0,\n",
       " 'randomforestregressor__min_impurity_split': None,\n",
       " 'randomforestregressor__min_samples_leaf': 1,\n",
       " 'randomforestregressor__min_samples_split': 2,\n",
       " 'randomforestregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestregressor__n_estimators': 100,\n",
       " 'randomforestregressor__n_jobs': None,\n",
       " 'randomforestregressor__oob_score': False,\n",
       " 'randomforestregressor__random_state': None,\n",
       " 'randomforestregressor__verbose': 0,\n",
       " 'randomforestregressor__warm_start': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list tunable hyperparameters:\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find a list of all the parameters on the [RandomForestRegressor documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Just note that when it's tuned through a pipeline, you'll need to prepend ```randomforestregressor__``` before the parameter name, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare hyperparameters to tune:\n",
    "hyperparameters = {'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'], \n",
    "                   'randomforestregressor__max_depth': [None, 5, 3, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format should be a Python dictionary where keys are the hyperparameter names and values are lists of settings to try. The options for parameter values can be found on the documentation page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model using a cross-validation pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation (CV) steps:**\n",
    "1. Split your data into $k$ equal parts, or \"folds\" (typically $k=10$).\n",
    "2. Train your model on $k-1$ folds (e.g. the first 9 folds).\n",
    "3. Evaluate it on the remaining \"hold-out\" fold (e.g. the 10th fold).\n",
    "4. Perform steps (2) and (3) k times, each time holding out a different fold.\n",
    "5. Aggregate the performance across all $k$ folds. This is your performance metric.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1280px-K-fold_cross_validation_EN.svg.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is cross-validation important in machine learning?**\n",
    "\n",
    "Using **only your training set**, you can use CV to evaluate different hyperparameters and estimate their effectiveness. This allows you to keep your test set \"untainted\" and save it for **a true hold-out evaluation** when you're finally ready to select a model.\n",
    "\n",
    "For example, you can use CV to tune a random forest model, a linear regression model, and a k-nearest neighbors model, using only the training set. Then, you still have the untainted test set to make your final selection between the model families!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a cross-validation pipeline?**\n",
    "\n",
    "The best practice when performing CV is to include your data preprocessing steps inside the cross-validation loop. This prevents accidentally tainting your training folds with influential data from your test fold.\n",
    "\n",
    "**Here's how the CV pipeline looks after including preprocessing steps:**\n",
    "1. Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "2. Preprocess k-1 training folds.\n",
    "3. Train your model on the same k-1 folds.\n",
    "4. Preprocess the hold-out fold using the same transformations from step (2).\n",
    "5. Evaluate your model on the same hold-out fold.\n",
    "6. Perform steps (2) - (5) k times, each time holding out a different fold.\n",
    "7. Aggregate the performance across all k folds. This is your performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('randomforestregressor',\n",
       "                                        RandomForestRegressor(bootstrap=True,\n",
       "                                                              ccp_alpha=0.0,\n",
       "                                                              criterion='mse',\n",
       "                                                              max_depth=None,\n",
       "                                                              max_features='auto',\n",
       "                                                              max_leaf_nodes=None,\n",
       "                                                              max_samples=None,\n",
       "                                                              min_impurity_decrease=0.0,\n",
       "                                                              min_impurity_...\n",
       "                                                              min_weight_fraction_leaf=0.0,\n",
       "                                                              n_estimators=100,\n",
       "                                                              n_jobs=None,\n",
       "                                                              oob_score=False,\n",
       "                                                              random_state=None,\n",
       "                                                              verbose=0,\n",
       "                                                              warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1],\n",
       "                         'randomforestregressor__max_features': ['auto', 'sqrt',\n",
       "                                                                 'log2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the cross-validation pipeline:\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    "\n",
    "# fit and tune model:\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Here, we only apply CV to ```X_train``` and ```y_train```. The purpose of CV is to evaluate and select the best hyperparameter values, not evaluate the performance of the entire model. Instead, we will use the test set (```X_test``` and ```y_test```) to do this later.\n",
    "\n",
    "**Note:**```GridSearchCV``` essentially performs cross-validation across the entire \"grid\" (all possible permutations) of hyperparameters. Now, you can see the best set of parameters found using CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': None,\n",
       " 'randomforestregressor__max_features': 'log2'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tip:*** *It turns out that in practice, random forests don't actually require a lot of tuning. They tend to work pretty well out-of-the-box with a reasonable number of trees. Even so, these same steps can be used when building any type of supervised learning model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refit on the entire training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've tuned your hyperparameters appropriately using cross-validation, you can generally get a small performance improvement by refitting the model on the entire training set.\n",
    "\n",
    "Conveniently, ```GridSearchCV``` from sklearn will automatically refit the model with the best set of hyperparameters using the entire training set.\n",
    "\n",
    "This functionality is ON by default, but you can confirm it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.refit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model pipeline on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47773012894243005\n",
      "0.337006875\n"
     ]
    }
   ],
   "source": [
    "# predict a new set of data:\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate the model with the imported metrics:\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to improve a model. We'll have more guides that go into detail about how to improve model performance, but here are a few quick things to try:\n",
    "1. Try other regression model families (e.g. regularized regression, boosted trees, etc.).\n",
    "2. Collect more data if it's cheap to do so.\n",
    "3. Engineer smarter features after spending more time on exploratory analysis.\n",
    "\n",
    "**Note**: When you try other families of models, we recommend using the same training and test set as you used to fit the random forest model. That's the best way to get a true apples-to-apples comparison between your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.28, 5.69, 4.97, 5.55, 6.38, 5.51, 5.06, 4.82, 5.01, 5.91, 5.32,\n",
       "       5.74, 5.66, 5.19, 5.78, 5.76, 6.58, 5.63, 5.72, 6.97, 5.46, 5.58,\n",
       "       4.99, 6.05, 5.91, 5.02, 5.43, 5.18, 6.15, 5.97, 5.93, 6.43, 5.99,\n",
       "       5.02, 4.99, 5.91, 5.05, 6.04, 5.05, 6.07, 4.93, 5.96, 6.59, 5.04,\n",
       "       6.21, 5.26, 5.63, 5.56, 5.11, 6.42, 6.06, 5.41, 5.84, 5.13, 5.61,\n",
       "       5.54, 5.34, 5.35, 4.98, 5.33, 5.34, 5.17, 5.02, 5.87, 5.93, 5.24,\n",
       "       6.37, 5.02, 5.06, 6.68, 5.67, 5.6 , 5.18, 5.02, 5.33, 5.98, 5.31,\n",
       "       5.08, 5.21, 5.29, 6.4 , 5.65, 6.07, 6.4 , 5.07, 6.16, 6.58, 6.28,\n",
       "       5.54, 5.65, 6.03, 5.52, 6.53, 5.7 , 5.76, 5.75, 6.75, 6.72, 5.56,\n",
       "       6.77, 5.07, 5.49, 5.12, 6.43, 5.07, 4.81, 5.72, 5.04, 5.72, 5.99,\n",
       "       5.8 , 5.48, 6.04, 5.38, 5.23, 5.2 , 5.92, 5.05, 5.06, 5.94, 5.85,\n",
       "       5.11, 5.79, 6.07, 5.29, 5.27, 5.3 , 5.93, 5.48, 5.34, 5.65, 6.21,\n",
       "       5.09, 5.32, 5.12, 6.38, 5.04, 5.15, 6.67, 5.52, 5.19, 5.01, 5.6 ,\n",
       "       6.03, 5.36, 5.33, 5.11, 6.43, 5.8 , 5.07, 5.58, 5.15, 4.95, 5.02,\n",
       "       5.24, 5.97, 5.35, 5.63, 5.77, 5.28, 5.56, 5.17, 5.29, 6.  , 5.  ,\n",
       "       5.98, 5.15, 5.46, 5.41, 5.15, 5.99, 5.07, 5.66, 5.07, 5.46, 5.45,\n",
       "       5.02, 5.37, 5.53, 5.04, 5.98, 5.54, 5.02, 5.03, 5.16, 6.13, 5.26,\n",
       "       5.65, 5.14, 4.97, 5.52, 6.55, 5.79, 5.8 , 5.42, 5.22, 5.45, 5.06,\n",
       "       6.23, 4.82, 6.16, 5.08, 5.22, 5.25, 6.8 , 5.97, 5.38, 5.22, 5.4 ,\n",
       "       5.94, 5.82, 5.93, 5.95, 6.28, 5.69, 5.99, 5.2 , 5.32, 5.7 , 5.29,\n",
       "       5.21, 5.9 , 6.08, 5.54, 6.15, 5.87, 5.61, 6.13, 5.48, 5.69, 5.43,\n",
       "       5.46, 6.1 , 5.71, 4.86, 4.52, 6.61, 6.42, 6.18, 5.2 , 5.39, 5.5 ,\n",
       "       5.37, 6.21, 6.  , 5.17, 5.08, 5.38, 5.29, 6.33, 5.18, 5.05, 5.22,\n",
       "       5.14, 5.89, 6.45, 5.7 , 5.5 , 5.53, 6.45, 5.62, 5.99, 5.25, 5.23,\n",
       "       5.75, 5.93, 5.76, 5.6 , 5.37, 5.18, 5.74, 5.51, 6.49, 6.11, 5.64,\n",
       "       4.95, 5.97, 6.5 , 5.89, 5.55, 5.59, 5.34, 5.35, 5.97, 6.92, 5.31,\n",
       "       6.43, 5.98, 5.41, 5.57, 5.65, 5.07, 5.11, 6.06, 5.81, 6.02, 6.04,\n",
       "       5.93, 5.33, 5.77, 5.51, 6.17, 5.63, 6.87, 6.8 , 5.89, 6.25, 5.01,\n",
       "       5.32, 5.91, 5.27, 5.33, 6.01, 6.55, 6.4 , 5.28, 5.56, 5.71, 6.12,\n",
       "       5.54])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model to a .pkl file:\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "\n",
    "# load model from .pkl file:\n",
    "clf2 = joblib.load('rf_regressor.pkl')\n",
    "clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
